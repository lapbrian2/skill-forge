---
phase: 01-llm-infrastructure
plan: 03
type: execute
wave: 2
depends_on: ["01-01", "01-02"]
files_modified:
  - src/app/api/discover/route.ts
  - src/app/api/generate/route.ts
  - src/app/api/validate/route.ts
  - src/hooks/use-llm-stream.ts
autonomous: true

must_haves:
  truths:
    - "POST /api/discover returns Zod-validated structured JSON for all 5 actions using zodOutputFormat"
    - "POST /api/generate streams spec markdown token-by-token via SSE using toReadableStream()"
    - "POST /api/validate uses Zod-validated structured output for LLM clarity scoring"
    - "All API routes return token usage metadata in responses"
    - "Frontend can consume streaming responses with the useLLMStream hook"
    - "Different API actions use appropriate models (Haiku for classify/validate, Sonnet for question/brief/features/architecture/generate)"
  artifacts:
    - path: "src/app/api/discover/route.ts"
      provides: "Refactored discover route using llmParse with zodOutputFormat for all 5 actions"
      exports: ["POST"]
    - path: "src/app/api/generate/route.ts"
      provides: "Streaming spec generation route using llmStream with toReadableStream()"
      exports: ["POST"]
    - path: "src/app/api/validate/route.ts"
      provides: "Refactored validation route using llmParse for LLM clarity scoring"
      exports: ["POST"]
    - path: "src/hooks/use-llm-stream.ts"
      provides: "React hook for consuming SSE streams from API routes"
      exports: ["useLLMStream"]
  key_links:
    - from: "src/app/api/discover/route.ts"
      to: "src/lib/llm/client.ts"
      via: "imports llmParse for structured output"
      pattern: "llmParse"
    - from: "src/app/api/discover/route.ts"
      to: "src/lib/llm/schemas.ts"
      via: "imports Zod schemas for each action"
      pattern: "ClassificationSchema|DiscoveryQuestionSchema|ProductBriefSchema|FeaturesSchema|ArchitectureSchema"
    - from: "src/app/api/generate/route.ts"
      to: "src/lib/llm/client.ts"
      via: "imports llmStream for streaming spec generation"
      pattern: "llmStream"
    - from: "src/app/api/generate/route.ts"
      to: "stream\\.toReadableStream\\(\\)"
      via: "converts MessageStream to SSE ReadableStream"
      pattern: "toReadableStream"
    - from: "src/hooks/use-llm-stream.ts"
      to: "@anthropic-ai/sdk"
      via: "Anthropic.MessageStream.fromReadableStream for SSE parsing"
      pattern: "MessageStream\\.fromReadableStream"
---

<objective>
Refactor all API routes to use the new LLM infrastructure (streaming, structured output, model selection, token tracking) and create the frontend stream consumer hook.

Purpose: This connects the infrastructure built in Plans 01+02 to the actual application. After this plan, the /api/generate endpoint streams spec markdown in real time (LLM-01), all structured outputs are Zod-validated (LLM-02), retry logic protects all calls (LLM-03), prompt caching is active (LLM-04), each action uses the right model (LLM-05), and token usage is returned for tracking (LLM-06).
Output: Three refactored API routes and one new React hook for streaming consumption
</objective>

<execution_context>
@C:/Users/Brian/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Brian/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-llm-infrastructure/01-RESEARCH.md

# Need summaries from Plans 01 and 02 to know exact exports
@.planning/phases/01-llm-infrastructure/01-01-SUMMARY.md
@.planning/phases/01-llm-infrastructure/01-02-SUMMARY.md

@src/app/api/discover/route.ts
@src/app/api/generate/route.ts
@src/app/api/validate/route.ts
@src/lib/llm/prompts.ts
@src/lib/constants.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor API routes to use new LLM infrastructure</name>
  <files>src/app/api/discover/route.ts, src/app/api/generate/route.ts, src/app/api/validate/route.ts</files>
  <action>
Refactor all three API routes to use the new llmParse() and llmStream() functions from client.ts, Zod schemas from schemas.ts, and model configuration from models.ts.

**`/api/discover/route.ts` -- replace llmCallJSON with llmParse:**

For each of the 5 action cases (classify, question, brief, features, architecture):
1. Remove the inline TypeScript generic type from llmCallJSON<...>()
2. Import the corresponding Zod schema from "@/lib/llm/schemas"
3. Replace `llmCallJSON<...>({...})` with `llmParse({ task: "classify", system: ..., prompt: ..., schema: ClassificationSchema })`
4. The result is `{ data, usage }` instead of `{ data, meta }`
5. Return `NextResponse.json({ ...data, meta: usage })` to maintain the same response shape for the frontend (rename usage fields to match existing meta format: tokens_input, tokens_output, model, plus new cache_read, cache_creation fields)

Map actions to task types:
- "classify" -> task: "classify", schema: ClassificationSchema, system: SYSTEM_COMPLEXITY
- "question" -> task: "question", schema: DiscoveryQuestionSchema, system: SYSTEM_DISCOVERY
- "brief" -> task: "brief", schema: ProductBriefSchema, system: SYSTEM_DISCOVERY
- "features" -> task: "features", schema: FeaturesSchema, system: SYSTEM_DISCOVERY
- "architecture" -> task: "architecture", schema: ArchitectureSchema, system: SYSTEM_DISCOVERY

Remove import of `llmCallJSON` from client. Import `llmParse` instead.
Remove import of `NextRequest` if not needed (use `Request` type from Web API).
Keep `NextResponse` for JSON responses.

**`/api/generate/route.ts` -- replace llmCall with llmStream for streaming:**

This is the most significant change. Convert from blocking JSON response to SSE streaming:
1. Remove import of `llmCall` from client. Import `llmStream` instead.
2. Remove `NextResponse` import -- use raw `Response` instead (NextResponse may add compression that breaks SSE).
3. Call `llmStream({ task: "generate", system: SYSTEM_GENERATOR, prompt: promptGenerateSpec(...) })`
4. The stream object has `.toReadableStream()` -- return it directly:
   ```typescript
   const stream = await llmStream({ task: "generate", system: SYSTEM_GENERATOR, prompt: ... });
   return new Response(stream.toReadableStream(), {
     headers: {
       "Content-Type": "text/event-stream",
       "Cache-Control": "no-cache",
       "Connection": "keep-alive",
     },
   });
   ```
5. Remove the section_count and word_count calculations -- these will be computed client-side from the streamed content.
6. The token usage will be available to the frontend via `stream.finalMessage().usage` on the client side (through the useLLMStream hook).

Keep the request body parsing and validation (project_data required check).

**`/api/validate/route.ts` -- replace llmCallJSON with llmParse for LLM clarity scoring:**

Only the LLM clarity scoring section (the optional `llmClarity` block near the end) needs updating:
1. Import `llmParse` from client and `ValidationClaritySchema` from schemas
2. Replace the `llmCallJSON<...>()` call with `llmParse({ task: "validate", system: SYSTEM_VALIDATOR, prompt: promptValidateClarity(...), schema: ValidationClaritySchema })`
3. Use `result.data` instead of destructured `{ data }`
4. The rest of the validation logic (Tollgate 4, Tollgate 5, scoring) stays exactly the same -- it's pure server-side logic, not LLM-dependent
5. Add token usage from the LLM clarity call to the response if available

Keep all existing non-LLM validation logic unchanged. The weasel word detection, section checking, placeholder scanning -- all pure functions that work correctly.

For all three routes, follow existing error handling pattern: try/catch with console.error("[/api/route]", message) and NextResponse.json({error}, {status: 500}).
  </action>
  <verify>
Run `npx tsc --noEmit` from project root. All three route files should compile without errors. Verify that:
- discover/route.ts imports llmParse and 5 schemas
- generate/route.ts imports llmStream and returns Response (not NextResponse)
- validate/route.ts imports llmParse and ValidationClaritySchema
- No remaining imports of deprecated llmCall or llmCallJSON in any route
  </verify>
  <done>
All three API routes use the new LLM infrastructure. /api/discover uses llmParse with Zod schemas for validated structured output. /api/generate streams spec markdown via SSE using llmStream + toReadableStream(). /api/validate uses llmParse for LLM clarity scoring. All routes include token usage in responses. No deprecated function imports remain in route files.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create frontend streaming consumer hook</name>
  <files>src/hooks/use-llm-stream.ts</files>
  <action>
Create `src/hooks/use-llm-stream.ts` -- a React hook that consumes SSE streams from the /api/generate endpoint (and any future streaming endpoints).

Implementation (based on verified research pattern from 01-RESEARCH.md):

1. Import `useState`, `useCallback`, `useRef` from "react"
2. Import `Anthropic` from "@anthropic-ai/sdk" for `MessageStream.fromReadableStream()`

3. Define `StreamState` interface:
   - text: string (accumulated streamed text)
   - isStreaming: boolean
   - error: string | null
   - usage: { input_tokens: number, output_tokens: number, cache_read_input_tokens: number, cache_creation_input_tokens: number, model: string } | null

4. Export `useLLMStream()` hook that returns:
   - text: string
   - isStreaming: boolean
   - error: string | null
   - usage: object | null
   - startStream: (url: string, body: object) => Promise<void>
   - abort: () => void

5. `startStream` implementation:
   a. Abort any existing stream via AbortController
   b. Create new AbortController, store in ref
   c. Reset state: text="", isStreaming=true, error=null, usage=null
   d. Fetch the URL with POST, JSON body, AbortController signal
   e. Check response.ok -- throw on non-200
   f. Create SDK stream: `Anthropic.MessageStream.fromReadableStream(response.body!)`
   g. Listen for text events: `sdkStream.on("text", (delta) => setState(prev => ({...prev, text: prev.text + delta})))`
   h. Await `sdkStream.finalMessage()` to get complete usage data
   i. On completion: set isStreaming=false, populate usage from finalMessage.usage
   j. On error: if AbortError, silently return. Otherwise set error message, isStreaming=false.

6. `abort` implementation:
   a. Call abortRef.current?.abort()
   b. Set isStreaming to false

Add "use client" directive at top of file since this uses React hooks.

Follow codebase conventions: named export, camelCase, proper TypeScript types. The hook should be clean and reusable for any streaming endpoint.

NOTE: The Anthropic SDK needs to be importable on the client side for `MessageStream.fromReadableStream()`. This is a static method that doesn't instantiate the full client. If bundle size becomes a concern later, this can be replaced with manual SSE parsing (documented in research as Option B), but start with the SDK approach for correctness.
  </action>
  <verify>
Run `npx tsc --noEmit` from project root. The hook file should compile without errors. Verify it exports useLLMStream with startStream and abort functions.
  </verify>
  <done>
useLLMStream hook provides complete streaming consumption: incremental text accumulation via SDK's MessageStream.fromReadableStream(), abort support via AbortController, usage data extraction from finalMessage(), and proper error handling. Ready for components to consume streaming API responses.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with zero errors across all modified/created files
2. `/api/discover` uses llmParse + Zod schemas (no more fragile JSON.parse)
3. `/api/generate` returns SSE stream (Content-Type: text/event-stream)
4. `/api/validate` uses llmParse for optional LLM scoring
5. `useLLMStream` hook exists and exports startStream + abort
6. No deprecated function imports remain in any API route
7. All LLM calls go through retry logic (inherited from llmParse/llmStream)
8. All LLM calls use prompt caching (inherited from llmParse/llmStream)
9. Each action uses the correct model via getModelConfig
</verification>

<success_criteria>
- POST /api/discover returns Zod-validated data for all 5 actions with correct per-action model selection
- POST /api/generate returns a streaming SSE response that can be consumed with useLLMStream
- POST /api/validate uses Zod-validated LLM scoring (optional section)
- useLLMStream hook accumulates text incrementally, provides abort, and captures usage data
- All 6 Phase 1 requirements (LLM-01 through LLM-06) are addressed at the infrastructure level
- TypeScript compiles with zero errors
</success_criteria>

<output>
After completion, create `.planning/phases/01-llm-infrastructure/01-03-SUMMARY.md`
</output>
