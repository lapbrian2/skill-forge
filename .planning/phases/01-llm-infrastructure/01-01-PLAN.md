---
phase: 01-llm-infrastructure
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/llm/schemas.ts
  - src/lib/llm/models.ts
  - src/lib/llm/client.ts
  - src/lib/constants.ts
autonomous: true

must_haves:
  truths:
    - "LLM client supports streaming via client.messages.stream() with toReadableStream()"
    - "LLM client supports structured output via client.messages.parse() with zodOutputFormat()"
    - "Failed LLM calls retry with exponential backoff up to 3 attempts"
    - "System prompts use cache_control for Anthropic prompt caching"
    - "Each LLM task type maps to an appropriate model (Haiku for fast tasks, Sonnet for complex)"
  artifacts:
    - path: "src/lib/llm/schemas.ts"
      provides: "Zod schemas for all structured LLM outputs (classify, question, brief, features, architecture, validate)"
      exports: ["ClassificationSchema", "DiscoveryQuestionSchema", "ProductBriefSchema", "FeaturesSchema", "ArchitectureSchema", "ValidationClaritySchema"]
    - path: "src/lib/llm/models.ts"
      provides: "Per-task model configuration map with model ID, maxTokens, and temperature"
      exports: ["MODEL_CONFIG", "TaskType", "getModelConfig"]
    - path: "src/lib/llm/client.ts"
      provides: "Refactored LLM client with streaming, structured output, retry, and prompt caching"
      exports: ["llmStream", "llmParse", "withRetry", "getClient"]
  key_links:
    - from: "src/lib/llm/client.ts"
      to: "src/lib/llm/models.ts"
      via: "import getModelConfig for per-task model selection"
      pattern: "getModelConfig\\("
    - from: "src/lib/llm/client.ts"
      to: "@anthropic-ai/sdk"
      via: "client.messages.stream() and client.messages.parse()"
      pattern: "client\\.messages\\.(stream|parse)"
    - from: "src/lib/llm/schemas.ts"
      to: "zod"
      via: "z.object schema definitions"
      pattern: "z\\.object"
---

<objective>
Build the core LLM infrastructure: Zod schemas for all structured outputs, per-task model configuration, and a refactored LLM client with streaming, structured output parsing, retry logic, and prompt caching.

Purpose: This is the foundation that all API routes will depend on. The current client uses non-streaming llmCall() with fragile JSON.parse and a single hardcoded model. This plan replaces it with production-grade infrastructure using the Anthropic SDK's native capabilities.
Output: Three files -- schemas.ts (Zod schemas), models.ts (model config map), client.ts (refactored with streaming + retry + caching)
</objective>

<execution_context>
@C:/Users/Brian/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Brian/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-llm-infrastructure/01-RESEARCH.md

@src/lib/llm/client.ts
@src/lib/llm/prompts.ts
@src/lib/constants.ts
@src/lib/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Zod schemas and model configuration</name>
  <files>src/lib/llm/schemas.ts, src/lib/llm/models.ts</files>
  <action>
Create `src/lib/llm/schemas.ts` with Zod v4 schemas for every structured LLM output in the app. Import from "zod" (v4.3.6 already installed). These schemas will be used with `zodOutputFormat()` to get validated structured output from the Anthropic SDK.

Schemas to define:
1. `ClassificationSchema` - z.object with: complexity (z.enum(["simple", "moderate", "complex"])), is_agentic (z.boolean()), reasoning (z.string()), suggested_name (z.string()), one_liner (z.string())
2. `DiscoveryQuestionSchema` - z.object with: question (z.string()), why (z.string()), options (z.array(z.string()).nullable()), field (z.string()), phase_complete (z.boolean())
3. `ProductBriefSchema` - z.object with: name (z.string()), display_name (z.string()), one_liner (z.string()), vision (z.string()), target_user (z.string()), platform (z.string()), timeline (z.string()), out_of_scope (z.array(z.string())), competitive (z.string()), is_agentic (z.boolean())
4. `FeaturesSchema` - z.object with: features (z.array of z.object with name, description, tier (z.enum), agent_role (z.enum), acceptance_criteria (z.array(z.string())), edge_cases (z.array(z.string())), error_handling (z.array of z.object with condition, handling, user_message)), user_stories (z.array(z.string())), nonfunctional (z.array(z.string()))
5. `ArchitectureSchema` - z.object with: data_model (z.string()), api_design (z.string()), tech_stack (z.string()), security (z.string()), agentic_architecture (z.string()), state_management (z.string())
6. `ValidationClaritySchema` - z.object with: scores (z.object with specificity, buildability, completeness, consistency -- all z.number()), overall (z.number()), issues (z.array(z.string())), suggestions (z.array(z.string()))

Export all schemas and their inferred types (using z.infer).

Create `src/lib/llm/models.ts` with per-task model configuration:
1. Define `TaskType` as: "classify" | "question" | "brief" | "features" | "architecture" | "generate" | "validate"
2. Define `ModelConfig` interface with: model (string), maxTokens (number), temperature (number)
3. Create `MODEL_CONFIG` record mapping each TaskType to its config:
   - classify: claude-haiku-4-5, 1024 tokens, 0.3 temp (fast, cheap for classification)
   - question: claude-sonnet-4-20250514, 2048 tokens, 0.7 temp (needs quality for contextual questions)
   - brief: claude-sonnet-4-20250514, 4096 tokens, 0.5 temp
   - features: claude-sonnet-4-20250514, 8192 tokens, 0.6 temp
   - architecture: claude-sonnet-4-20250514, 8192 tokens, 0.5 temp
   - generate: claude-sonnet-4-20250514, 16384 tokens, 0.5 temp (long form spec generation)
   - validate: claude-haiku-4-5, 2048 tokens, 0.2 temp (fast, deterministic scoring)
4. Export a `getModelConfig(task: TaskType): ModelConfig` helper function

Follow codebase conventions: section header comments with decorative dividers, named exports, SCREAMING_SNAKE_CASE for the config constant.
  </action>
  <verify>
Run `npx tsc --noEmit` from project root. Both files should compile without errors. Verify schemas.ts exports 6 schemas and models.ts exports MODEL_CONFIG with 7 entries.
  </verify>
  <done>
schemas.ts contains 6 Zod schemas matching the exact shape of every structured LLM response in the app. models.ts contains MODEL_CONFIG with appropriate model/maxTokens/temperature for each of 7 task types. Both files compile cleanly.
  </done>
</task>

<task type="auto">
  <name>Task 2: Refactor LLM client with streaming, retry, and prompt caching</name>
  <files>src/lib/llm/client.ts, src/lib/constants.ts</files>
  <action>
Refactor `src/lib/llm/client.ts` to replace the current `llmCall()` and `llmCallJSON()` with production-grade functions. Keep the existing `getClient()` singleton pattern.

Add these new exported functions:

1. `withRetry<T>(fn: () => Promise<T>, maxAttempts?: number, baseDelayMs?: number): Promise<T>`
   - Default maxAttempts = 3, baseDelayMs = 1000
   - On each failure: check if error is Anthropic.APIError; do NOT retry 4xx errors except 429 (rate limit)
   - Delay formula: baseDelayMs * Math.pow(2, attempt) + Math.random() * 500
   - Re-throw last error if all attempts fail
   - Import Anthropic type from "@anthropic-ai/sdk" to check error types

2. `llmStream(options: LLMStreamOptions): MessageStream`
   - LLMStreamOptions: { task: TaskType, system: string, prompt: string, messages?: array }
   - Uses getModelConfig(task) for model, maxTokens, temperature
   - Calls client.messages.stream() with the config
   - Adds cache_control: { type: "ephemeral" } at the top level for prompt caching
   - Wraps the stream creation in withRetry (note: retry wraps the stream creation, not the consumption)
   - Returns the MessageStream directly (caller converts to ReadableStream or consumes events)
   - Return type: use the SDK's MessageStream type from "@anthropic-ai/sdk/lib/MessageStream"

3. `llmParse<T extends z.ZodType>(options: LLMParseOptions<T>): Promise<LLMParseResult<T>>`
   - LLMParseOptions: { task: TaskType, system: string, prompt: string, schema: T, messages?: array }
   - Uses getModelConfig(task) for model, maxTokens, temperature
   - Import zodOutputFormat from "@anthropic-ai/sdk/helpers/zod"
   - Calls client.messages.parse() with output_config: { format: zodOutputFormat(schema) }
   - Adds cache_control: { type: "ephemeral" } for prompt caching
   - Wraps in withRetry
   - Returns { data: z.infer<T>, usage: { input_tokens, output_tokens, cache_read_input_tokens, cache_creation_input_tokens, model } }
   - LLMParseResult<T> interface: { data: z.infer<T>, usage: TokenUsageData }

4. Define `TokenUsageData` interface: { input_tokens: number, output_tokens: number, cache_read_input_tokens: number, cache_creation_input_tokens: number, model: string }

Keep the old `llmCall()` and `llmCallJSON()` functions but mark them as `@deprecated` with JSDoc comments pointing to `llmStream` and `llmParse`. Do NOT delete them yet -- the API routes still use them until Plan 02 migrates them. This avoids breaking the app during Plan 01.

In `src/lib/constants.ts`: Keep the existing LLM_MODEL, LLM_MAX_TOKENS, LLM_TEMPERATURE constants (they're still used by the deprecated functions). Add a comment: "// DEPRECATED: Use MODEL_CONFIG from src/lib/llm/models.ts instead"

Import considerations:
- `import Anthropic from "@anthropic-ai/sdk"` (default import for client + error types)
- `import { zodOutputFormat } from "@anthropic-ai/sdk/helpers/zod"` (helper import -- note the subpath!)
- `import { z } from "zod"`
- `import { getModelConfig, type TaskType } from "./models"`

Follow codebase conventions: section headers with decorative dividers, named exports, JSDoc for public functions, error handling with `error instanceof Error`.
  </action>
  <verify>
Run `npx tsc --noEmit` from project root. client.ts should compile without errors. Verify that old llmCall and llmCallJSON still exist (deprecated but functional). Verify new functions llmStream, llmParse, withRetry are exported.
  </verify>
  <done>
client.ts exports llmStream (streaming), llmParse (structured output with Zod validation), and withRetry (exponential backoff). Old functions marked deprecated but still functional. Prompt caching enabled via cache_control. Model selection delegated to models.ts. TypeScript compiles cleanly.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with zero errors
2. `src/lib/llm/schemas.ts` exists with 6 exported Zod schemas
3. `src/lib/llm/models.ts` exists with MODEL_CONFIG containing 7 task types
4. `src/lib/llm/client.ts` exports llmStream, llmParse, withRetry alongside deprecated llmCall, llmCallJSON
5. No existing functionality is broken (deprecated functions still work)
</verification>

<success_criteria>
- Zod schemas match the exact shape of every structured LLM response currently defined inline in API routes
- Model config maps classify and validate to Haiku, all other tasks to Sonnet
- llmStream returns an Anthropic MessageStream with toReadableStream() available
- llmParse returns typed, Zod-validated data with token usage metadata
- withRetry handles exponential backoff, skips 4xx errors (except 429)
- Prompt caching enabled on all LLM calls via cache_control
- TypeScript compiles with zero errors
</success_criteria>

<output>
After completion, create `.planning/phases/01-llm-infrastructure/01-01-SUMMARY.md`
</output>
